{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Grad School Notes","text":""},{"location":"#supervised-learning","title":"Supervised Learning","text":"<p>Function approximation from examples Goal: Learn a function  y = f(x)  from labeled data Given: Pairs of inputs and outputs (x, y) Learn: A function  f(x)  that maps inputs to outputs and generalizes to unseen  x  Examples: classification, regression</p>"},{"location":"#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Clustering, description, compression Goal: Learn data structure and description from input data alone Given: Inputs  x  Learn: A function  f(x)  that captures structure, clusters, or reduces dimensionality  </p>"},{"location":"#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Trial and error with feedback Goal: Learn how to make decisions Given: States  x , actions  y , and delayed rewards  z  Learn: A function or policy  y = f(x)  that maps states to actions to maximize reward Examples: games, robotics, recommendation systems</p>"},{"location":"#topics-so-far","title":"Topics so far","text":"<ul> <li>Machine Learning </li> <li>Natural Language Processing </li> <li>Deep Learning </li> <li>AI for Robotics </li> <li>Reinforced Learning </li> </ul>"},{"location":"#future-projects","title":"Future Projects","text":"<p>When I finish grad school - Junk Yard: Auto parts data base: Add all relevant auto parts in the world to database capturing key info like, photo, size, weight, id numbers - Junk Yard: Input live photo or video into CV component connected to database to access entry for that part - Stocks: Gather fresh data to plug into Stock Predictor  </p>"},{"location":"ml/","title":"Machine Learning","text":"<p>This will include topics like: - Supervised learning - Unsupervised learning - Bias-variance tradeoff</p> <p>```python</p>"},{"location":"ml/#example-linear-regression-formula","title":"Example: linear regression formula","text":"<p>y = X @ w + b</p> <p>y = mx + b</p>"},{"location":"nlp/distributional_semantics/","title":"Distributional Semantics","text":"<ul> <li>Word embeddings</li> <li>Word2Vec and Skip-gram</li> <li>CBOW</li> <li>GloVe</li> <li>Cosine similarity</li> <li>Analogy tasks</li> <li>Contextual embeddings (BERT, ELMo, etc.)</li> </ul>"},{"location":"nlp/distributional_semantics/#continuous-bag-of-words-cbow","title":"Continuous Bag of Words (CBOW)","text":"<p>Predict a single word based on surrounding words.</p> <p>'crown with the ___ on the throne'</p> <p>Loss: Cross entropy between the predicted distribution and the true center word (i.e., the word that was masked out).</p>"},{"location":"nlp/distributional_semantics/#skip-gram","title":"Skip-gram","text":"<p>Predict surrounding words based on single word.</p> <p>'___ ___ ___ queen ___ ___ ___'</p> <p>Loss: Sum of cross entropy losses for each context word given the center word.</p>"},{"location":"nlp/distributional_semantics/#word2vec","title":"Word2Vec","text":"<pre><code>class CBOW(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(CBOW, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: list of indices (window*2)\n    \"\"\"\n    probs = None\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = output.mean(dim=1)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre> <pre><code>class SkipGram(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(SkipGram, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: a single token index\n    Returns: probs: log softmax distro over the vocabulary\n    \"\"\"\n    probs = None\n    ### BEGIN SOLUTION\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre>"},{"location":"patterns/","title":"Patterns","text":"<p>Reusable code and structures used across domains.</p>"},{"location":"patterns/#docstring-templates","title":"Docstring templates","text":""},{"location":"patterns/#function-template-google-style","title":"Function Template (Google Style)","text":"<pre><code>def my_function(arg1, arg2):\n    \"\"\"Brief summary of function.\n\n    Args:\n        arg1 (type): Description of first argument. (shape)\n        arg2 (type): Description of second argument. (shape)\n\n    Returns:\n        type: Description of the return value.\n\n    Raises:\n        SomeError: When this happens.\n    \"\"\"\n</code></pre>"},{"location":"patterns/#class-template","title":"Class Template","text":"<pre><code>class MyModel:\n    \"\"\"Short summary of class.\n\n    Attributes:\n        name (str): Description of the attribute.\n        layers (list): Description of the model layers.\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Initialize the model with a name.\"\"\"\n        self.name = name\n</code></pre>"},{"location":"patterns/#general-hyperparameter-tuning-strategy","title":"General Hyperparameter Tuning Strategy","text":"<p>Improve model performance by adjusting key training and model configuration parameters without overfitting or wasting compute.</p> <p>Start with  baseline: - Run model with default or reasonable parameter - Record metrics: accuracy, loss, runtime - Use this to compare future improvements as we gradually increase complexity - Track experiments with Weights &amp; Biases  </p>"},{"location":"patterns/#tuning-order-priority","title":"Tuning Order Priority","text":"<ol> <li> <p>Learning rate</p> <ul> <li>Start with learning rate finder  </li> <li>Often has largest effect on both convergence speed and final performance  </li> </ul> </li> <li> <p>Batch Size  </p> <ul> <li>Larger batches: more stable gradients but may generalize worse  </li> <li>Smaller batches: more noise, sometimes better generalization</li> <li>Changing batch size may require retuning learning rate  </li> </ul> </li> <li> <p>Architecture/model capacity: decide if model too big/small  </p> <ul> <li>Tune only after learning is stable    </li> <li>Adjust number of layers, hidden units, width vs. depth    </li> </ul> </li> <li> <p>Regularization  </p> <ul> <li>Dropout, weight decay, early stopping, etc.</li> <li>After we have a model that can overfit out data  </li> </ul> </li> <li> <p>Optimizer     _ Different optimizers work better for different problems</p> <ul> <li>Adam is good default </li> <li>Try SGD+momentum is better for CV and large-scale tasks</li> </ul> </li> <li> <p>Advanced hyperparams specific to arch. (attention heads, embedding dims.)</p> </li> </ol>"},{"location":"patterns/#convergence","title":"Convergence","text":"<p>The point where the model stops meaningfully improving </p> <p>Signals of Convergence - Validation loss stops decreasing or begins to increase - Evaluation metric (e.g., accuracy, F1) stops improving - Training loss decreases but validation loss worsens: overfitting - Model predictions stabilize (low variance across epochs)</p> <p>Best Practices - Use early stopping (e.g., patience = 5 epochs without improvement) - Be cautious of slow convergence caused by learning rate that\u2019s too low  </p> <p>Practical Convergence Criteria - Task-specific thresholds that define \u201cgood enough\u201d - Helps guide whether to continue training, stop early, or try a different approach  </p> <p>Examples - \"Reach 80% accuracy in under 5 epochs\" - \"Improve F1 score to 0.7 using fewer than 100K parameters\" - \"Train for no more than 30 minutes on a laptop GPU\" - \"Outperform logistic regression by 5%\"</p>"},{"location":"patterns/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Accuracy - Percentage of correct predictions - Best for: balanced datasets - Bad for: imbalanced classes (e.g., 95% accuracy when 95% of data is class A = misleading)</p> <p>Precision - Of predicted positives, how many were correct? - Formula: <code>TP / (TP + FP)</code> - Best for: when false positives are costly (e.g., spam, security alerts)</p> <p>Recall - Of actual positives, how many were correctly predicted? - Formula: <code>TP / (TP + FN)</code> - Best for: when false negatives are costly (e.g., cancer and fraud detection)</p> <p>F1 Score - Harmonic mean of precision and recall - Formula: <code>2 * (P * R) / (P + R)</code> - Best for: imbalanced data where you care about both P and R</p> <p>AUC-ROC - Probability the model ranks a positive higher than a negative - Best for: binary classification, threshold-based tasks</p> <p>Perplexity - Measures how well a language model predicts a sample - Lower is better: lower perplexity means the model is more confident and assigns higher probability to the correct words  - So, Perplexity of 1 means perfect prediction (model is certain). Perplexity of 10 means model is choosing among ~10 likely options per token. - Best for: language models trained for next-word or sequence prediction (e.g., RNNs, Transformers)</p> <p>\\(   \\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i)} \\)  </p> <p>MSE / MAE / RMSE - Error metrics for continuous outputs - Best for: regression problems</p>"},{"location":"patterns/#batching","title":"Batching","text":"<p>Basic batching</p> <pre><code>def get_batch(data, index, batch_size=10):\n    \"\"\" \n    Create batch of data of given batch_size, starting at given index.\n    Check device, run on GPU if available. \n\n    Args: \n        data: CBOW data, list of (context_indices, target_index) tuples (2 * window, int)\n\n    Returns:\n        x: tensor of context words, one row per training example, each with it\n            context word indices, (batch_size, window * 2)\n        y: tensor of target words, one target word index per row (batch_size,)\n    \"\"\"\n    batch = data[index:index + batch_size]\n    x = [row[0] for row in batch]\n    y = [row[1] for row in batch]\n\n    x = torch.tensor(x, dtype=torch.long).to(DEVICE)\n    y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n    return x, y\n</code></pre>"}]}