{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Grad School Notes","text":""},{"location":"#supervised-learning","title":"Supervised Learning","text":"<p>Function approximation from examples Goal: Learn a function  y = f(x)  from labeled data Given: Pairs of inputs and outputs (x, y) Learn: A function  f(x)  that maps inputs to outputs and generalizes to unseen  x  Examples: classification, regression</p>"},{"location":"#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Clustering, description, compression Goal: Learn data structure and description from input data alone Given: Inputs  x  Learn: A function  f(x)  that captures structure, clusters, or reduces dimensionality  </p>"},{"location":"#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Trial and error with feedback Goal: Learn how to make decisions Given: States  x , actions  y , and delayed rewards  z  Learn: A function or policy  y = f(x)  that maps states to actions to maximize reward Examples: games, robotics, recommendation systems</p>"},{"location":"#topics-so-far","title":"Topics so far","text":"<ul> <li>Machine Learning </li> <li>Natural Language Processing </li> <li>Deep Learning </li> <li>AI for Robotics </li> <li>Reinforced Learning </li> </ul>"},{"location":"#future-projects","title":"Future Projects","text":"<p>When I finish grad school</p>"},{"location":"#junk-yard","title":"Junk Yard","text":"<p>Auto parts database: Add all relevant auto parts in the world to database capturing key info like, photo, size, weight, id numbers. - How do we define relevant? We need to find most popular reused parts. Based on car model? Car date? Parts used across models? - What about rare parts? Should we always keep since the world is running out of them?  </p> <p>Computer Vision: Input live photo or video into CV component connected to database to access entry for that part - Stocks: Gather fresh data to plug into Stock Predictor  </p> <p>Note: Keep notes on all steps in research spike.</p>"},{"location":"ml/","title":"Machine Learning","text":""},{"location":"ml/decision_trees/","title":"Decision Trees Overview","text":"<p>Decision Trees are a form of classification learning. Decision Trees are representations of our features and we need to determine representation before deciding on algo. First we identify our problem (root question). Then we identify attributes/features and ask questions about those. These are the decision nodes. Examples: Is it raining? What type of restaurant is it? </p> <p>Edges represent the value of each decision node. Examples: Yes and No. Pizza, Thai, Mexican. </p> <p>The leaf is the answer to the problem (root question), output. Example: Should we each in this restaurant? Should I play tennis? Leaf is yes or no.</p> <p>We need to build a decision tree as a result of processing training data. The order of our decision nodes should correlate to the node's ability to reduce space (number of nodes?). All nodes/features may not be necessary to make our root decision. If we only want to play tennis if its not raining, that might be near the top of our tree as it can rule out other branches and other features.</p> <p>Naive Decision Tree algo (to build a tree) 1. Pick the \"best\" feature \u2014 ideally one that splits the data in half or reduces entropy the most 2. Ask a question based on this feature 3. Follow the answer path 4. If not a leaf node, go back to step 1 with the remaining features and data subset.</p> <p>Problem here is that we have to follow all possible paths and think of all possible best next features until we can completely answer any question. So we aren't learning the tree, we are using a brute-force approach to build it.</p> <p>Given n boolean features, there are \\(2^n\\) possible ways to arrange the features and \\(2^{2^n}\\) ways to assign binary labels to those combinations. The hypothesis space (\\(2^{2^n}\\)) of the decision tree is very expressive because there are many possible functions a decision tree could represent: it has the capacity to represent a massive number of different functions from inputs to outputs.</p>"},{"location":"ml/decision_trees/#id3-algo","title":"ID3 Algo","text":"<p>Loop: 1. A = 'best' feature based on Information Gain. 2. Assign A as decision feature at this node (ask a question) 3. For each \\(v \\in A\\), create a branch from current node 4. Group the training examples where feature A = v into the corresponding branch. 5. If all examples in a branch are of the same class (pure leaf), stop 6.  Else, repeat the process recursively on that branch with remaining features.</p> <p>Example: A is Weather, values are {Sunny, Rainy, Overcast}. A becomes, \"What is the Weather?\" and \\(v \\in A\\) is one of the possible answers.</p> <p>Information Gain measures how much knowing the value of a feature helps us predict the label of a data point. - Before the split on a feature, our data might be a mix of labels (50% Yes, 50% No ~ high uncertainty) - After the split, if a feature separates the data such that one side is mostly Yes and the other mostly No, we\u2019ve reduced our uncertainty about the label by asking about that feature.  </p> <p>That's the whole goal of decision trees: to use features to reduce label uncertainty step by step.</p> <p>\\( \\text{Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v) \\)</p> <p>HI \\( a^2 + b^2 = c^2 \\)</p> <p>Where: - S is current set of training examples - A is the feature we are evaluating (Weather) - Values(A) are the possible values of feature A ({Sunny, Rainy, Overcast}) - S_v is subset of S where feature A = v - Entropy(S) is the entropy of the full set</p> <p>Everything after the minus sign in the Information Gain formula represents the expected entropy after the split \u2014 that is, the average uncertainty over each subset, weighted by the proportion of examples in that subset. This text is red</p>"},{"location":"nlp/distributional_semantics/","title":"Distributional Semantics","text":"<ul> <li>Word embeddings</li> <li>Word2Vec and Skip-gram</li> <li>CBOW</li> <li>GloVe</li> <li>Cosine similarity</li> <li>Analogy tasks</li> <li>Contextual embeddings (BERT, ELMo, etc.)</li> </ul>"},{"location":"nlp/distributional_semantics/#continuous-bag-of-words-cbow","title":"Continuous Bag of Words (CBOW)","text":"<p>Predict a single word based on surrounding words.</p> <p>'crown with the ___ on the throne'</p> <p>Loss: Cross entropy between the predicted distribution and the true center word (i.e., the word that was masked out).</p>"},{"location":"nlp/distributional_semantics/#skip-gram","title":"Skip-gram","text":"<p>Predict surrounding words based on single word.</p> <p>'___ ___ ___ queen ___ ___ ___'</p> <p>Loss: Sum of cross entropy losses for each context word given the center word.</p>"},{"location":"nlp/distributional_semantics/#word2vec","title":"Word2Vec","text":"<pre><code>class CBOW(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(CBOW, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: list of indices (window*2)\n    \"\"\"\n    probs = None\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = output.mean(dim=1)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre> <pre><code>class SkipGram(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(SkipGram, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: a single token index\n    Returns: probs: log softmax distro over the vocabulary\n    \"\"\"\n    probs = None\n    ### BEGIN SOLUTION\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre>"},{"location":"patterns/","title":"Patterns","text":"<p>Reusable code and structures used across domains.</p>"},{"location":"patterns/#docstring-templates","title":"Docstring templates","text":""},{"location":"patterns/#function-template-google-style","title":"Function Template (Google Style)","text":"<pre><code>def my_function(arg1, arg2):\n    \"\"\"Brief summary of function.\n\n    Args:\n        arg1 (type): Description of first argument. (shape)\n        arg2 (type): Description of second argument. (shape)\n\n    Returns:\n        type: Description of the return value.\n\n    Raises:\n        SomeError: When this happens.\n    \"\"\"\n</code></pre>"},{"location":"patterns/#class-template","title":"Class Template","text":"<pre><code>class MyModel:\n    \"\"\"Short summary of class.\n\n    Attributes:\n        name (str): Description of the attribute.\n        layers (list): Description of the model layers.\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Initialize the model with a name.\"\"\"\n        self.name = name\n</code></pre>"},{"location":"patterns/#general-hyperparameter-tuning-strategy","title":"General Hyperparameter Tuning Strategy","text":"<p>Improve model performance by adjusting key training and model configuration parameters without overfitting or wasting compute.</p> <p>Start with  baseline: - Run model with default or reasonable parameter - Record metrics: accuracy, loss, runtime - Use this to compare future improvements as we gradually increase complexity - Track experiments with Weights &amp; Biases  </p>"},{"location":"patterns/#tuning-order-priority","title":"Tuning Order Priority","text":"<ol> <li> <p>Learning rate</p> <ul> <li>Start with learning rate finder  </li> <li>Often has largest effect on both convergence speed and final performance  </li> </ul> </li> <li> <p>Batch Size  </p> <ul> <li>Larger batches: more stable gradients but may generalize worse  </li> <li>Smaller batches: more noise, sometimes better generalization</li> <li>Changing batch size may require retuning learning rate  </li> </ul> </li> <li> <p>Architecture/model capacity: decide if model too big/small  </p> <ul> <li>Tune only after learning is stable    </li> <li>Adjust number of layers, hidden units, width vs. depth    </li> </ul> </li> <li> <p>Regularization  </p> <ul> <li>Dropout, weight decay, early stopping, etc.</li> <li>After we have a model that can overfit out data  </li> </ul> </li> <li> <p>Optimizer     _ Different optimizers work better for different problems</p> <ul> <li>Adam is good default </li> <li>Try SGD+momentum is better for CV and large-scale tasks</li> </ul> </li> <li> <p>Advanced hyperparams specific to arch. (attention heads, embedding dims.)</p> </li> </ol>"},{"location":"patterns/#convergence","title":"Convergence","text":"<p>The point where the model stops meaningfully improving </p> <p>Signals of Convergence - Validation loss stops decreasing or begins to increase - Evaluation metric (e.g., accuracy, F1) stops improving - Training loss decreases but validation loss worsens: overfitting - Model predictions stabilize (low variance across epochs)</p> <p>Best Practices - Use early stopping (e.g., patience = 5 epochs without improvement) - Be cautious of slow convergence caused by learning rate that\u2019s too low  </p> <p>Practical Convergence Criteria - Task-specific thresholds that define \u201cgood enough\u201d - Helps guide whether to continue training, stop early, or try a different approach  </p> <p>Examples - \"Reach 80% accuracy in under 5 epochs\" - \"Improve F1 score to 0.7 using fewer than 100K parameters\" - \"Train for no more than 30 minutes on a laptop GPU\" - \"Outperform logistic regression by 5%\"</p>"},{"location":"patterns/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Accuracy - Percentage of correct predictions - Best for: balanced datasets - Bad for: imbalanced classes (e.g., 95% accuracy when 95% of data is class A = misleading)</p> <p>Precision - Of predicted positives, how many were correct? - Formula: <code>TP / (TP + FP)</code> - Best for: when false positives are costly (e.g., spam, security alerts)</p> <p>Recall - Of actual positives, how many were correctly predicted? - Formula: <code>TP / (TP + FN)</code> - Best for: when false negatives are costly (e.g., cancer and fraud detection)</p> <p>F1 Score - Harmonic mean of precision and recall - Formula: <code>2 * (P * R) / (P + R)</code> - Best for: imbalanced data where you care about both P and R</p> <p>AUC-ROC - Probability the model ranks a positive higher than a negative - Best for: binary classification, threshold-based tasks</p> <p>Perplexity - Measures how well a language model predicts a sample - Lower is better: lower perplexity means the model is more confident and assigns higher probability to the correct words  - So, Perplexity of 1 means perfect prediction (model is certain). Perplexity of 10 means model is choosing among ~10 likely options per token. - Best for: language models trained for next-word or sequence prediction (e.g., RNNs, Transformers)</p> <p>\\(   \\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i)} \\)  </p> <p>MSE / MAE / RMSE - Error metrics for continuous outputs - Best for: regression problems</p>"},{"location":"patterns/#batching","title":"Batching","text":"<p>Basic batching</p> <pre><code>def get_batch(data, index, batch_size=10):\n    \"\"\" \n    Create batch of data of given batch_size, starting at given index.\n    Check device, run on GPU if available. \n\n    Args: \n        data: CBOW data, list of (context_indices, target_index) tuples (2 * window, int)\n\n    Returns:\n        x: tensor of context words, one row per training example, each with it\n            context word indices, (batch_size, window * 2)\n        y: tensor of target words, one target word index per row (batch_size,)\n    \"\"\"\n    batch = data[index:index + batch_size]\n    x = [row[0] for row in batch]\n    y = [row[1] for row in batch]\n\n    x = torch.tensor(x, dtype=torch.long).to(DEVICE)\n    y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n    return x, y\n</code></pre>"},{"location":"patterns/glossary/","title":"Glossary","text":""},{"location":"patterns/glossary/#polynomial-time","title":"Polynomial Time","text":"<p>When we say an algorithm runs in polynomial time, we mean that:</p> <p>Its worst-case running time can be expressed as a polynomial function of the input size.</p> <p>Like the following, where $ n $ = input size:</p> <ul> <li>O(n) \u2013 linear time  </li> <li>O(n^2) \u2013 quadratic time  </li> <li>O(n^3) \u2013 cubic time  </li> <li>...  </li> <li>O(n^k) \u2013 for some constant k</li> </ul> <p>Polynomial time is considered efficient. It contrasts with exponential time algorithms, like O(2^n), which become intractable as n grows.</p>"},{"location":"patterns/stats/","title":"Probability &amp; Statistics","text":""},{"location":"patterns/stats/#statistics","title":"Statistics","text":""},{"location":"patterns/stats/#moving-average","title":"Moving Average","text":"<p>A moving average is a way to smooth out noisy data by averaging over a sliding window of points. We lose the first and last few points, but the result is a smoother version of the original signal. Output = N - W + 1. So the first W - 1 data points don\u2019t get included in the result (no \"look-behind\"). A moving average smooths the data by averaging out short-term fluctuations. It dampens highs and lows, suppressing noise.</p> <p>Imagine you have this list of episode rewards:</p> <p>data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</p> <p>If you choose a window size of 3, here\u2019s what happens:</p> <p>Take the average of the first 3 numbers: (1 + 2 + 3) / 3 = 2</p> <p>Slide the window one step and average the next 3: (2 + 3 + 4) / 3 = 3</p> <p>Keep doing this: (3 + 4 + 5) / 3 = 4 (4 + 5 + 6) / 3 = 5 ...</p> <p>[2, 3, 4, 5, 6, 7, 8, 9]</p> <p>Why Use It? In reinforcement learning, the reward per episode can bounce all over the place due to randomness. A moving average helps you see the underlying trend by smoothing out that variance.</p> <p>np.convolve(data, np.ones(window_size) / window_size, mode='valid')</p>"},{"location":"rl/","title":"Overview","text":"<p>Q-Values represent expected cumulative reward for taking a specific action in a given state.</p> <p>Q-values represent the expected cumulative reward for taking a specific action in a given state, and then following the policy thereafter. So:</p>  Q(s, a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid s_0 = s, a_0 = a \\right]  <p>Where: -  s  is the current state -  a  is the action taken -  \\gamma  is the discount factor -  R_t  are the rewards</p> <p>So it\u2019s about both state and action (not just the state).</p> <p>epsilon decay ration Even if we think a certain action has the highest Q-Value(based on earlier exploration) in a state, we still randomly try other actions with probability epsilon. This helps us discover better actions we might have missed early on.</p> <p>alpha: learning rate Controls how much we update the Q-value based on new information. Higher values is bigger updates to Q-values.</p> <p>Epsilon controls which actions we try (explore).</p> <p>Alpha controls how strongly each experience influences learning. </p>  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\big[ \\text{target} - Q(s, a) \\big]"},{"location":"rl/game_theory/","title":"Game Theory","text":"<p>perfect information: we always know what state we are in</p> <p>strategies: mapping of all possible states to actions. Each agent has their own strategy.</p>"}]}