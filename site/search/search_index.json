{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Grad School Notes","text":""},{"location":"#supervised-learning","title":"Supervised Learning","text":"<p>Function approximation from examples Goal: Learn a function  y = f(x)  from labeled data Given: Pairs of inputs and outputs (x, y) Learn: A function  f(x)  that maps inputs to outputs and generalizes to unseen  x  Examples: classification, regression</p>"},{"location":"#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Clustering, description, compression Goal: Learn data structure and description from input data alone Given: Inputs  x  Learn: A function  f(x)  that captures structure, clusters, or reduces dimensionality  </p>"},{"location":"#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Trial and error with feedback Goal: Learn how to make decisions Given: States  x , actions  y , and delayed rewards  z  Learn: A function or policy  y = f(x)  that maps states to actions to maximize reward Examples: games, robotics, recommendation systems</p>"},{"location":"#topics-so-far","title":"Topics so far","text":"<ul> <li>Machine Learning </li> <li>Natural Language Processing </li> <li>Deep Learning </li> <li>AI for Robotics </li> <li>Reinforced Learning </li> </ul>"},{"location":"#future-projects","title":"Future Projects","text":"<p>When I finish grad school</p>"},{"location":"#junk-yard","title":"Junk Yard","text":"<p>Auto parts database: Add all relevant auto parts in the world to database capturing key info like, photo, size, weight, id numbers. - How do we define relevant? We need to find most popular reused parts. Based on car model? Car date? Parts used across models? - What about rare parts? Should we always keep since the world is running out of them?  </p> <p>Computer Vision: Input live photo or video into CV component connected to database to access entry for that part - Stocks: Gather fresh data to plug into Stock Predictor  </p>"},{"location":"#test-bed","title":"Test Bed","text":"<p>Set up test env and dataset(s) so I can easily test improvements (of models, loss functions, optimizers, privacy, data transforms etc.). Create bench marking in a variety of metrics. Save model params and optimizer. Saving optimizer allows us to continue training where we left off.</p> <p>Note: Keep notes on all steps in research spike.</p>"},{"location":"ml/","title":"Machine Learning","text":"<p>An inductive learner uses examples to find the best hypothesis h* from some class H. H is the set of possible answers to the problem. Think of hypothesis h as candidate answers.</p>"},{"location":"ml/#computation-learning-theory","title":"Computation Learning Theory","text":"<p>log base 2, is like keep dividing by 2</p>"},{"location":"ml/#pac-learning-probably-approximately-correct-error-of-hypothesis-h","title":"PAC Learning : Probably Approximately Correct : Error of Hypothesis h","text":"<p>Training Error: Fraction of training examples misclassified by h. Target concept should have training error of zero. \\(error_d (h_i) &gt; \\epsilon\\) means h is wrong, aka  \\( h_i \\neq c_i \\)</p> <p>True Error: Fraction of examples that would be misclassified on sample drawn from D in, essentially, the infinite limit. The probability that a sample drawn from D would be misclassified by some hypothesis h. </p> <p>\\(  error_d(h) = P(c(x) \\neq h(x))\\) in x~D distribution </p> <p>So we are not penalized for examples that we never see. And examples that we see rarely, we only get a time bit of contribution to the overall error.</p> <p>C is PAC-Learnable by L using H, \\(iff\\) learning L will, with high probability ( at least \\(  1-\\delta\\)), output a hypothesis h \\(\\in\\) H that it is very accurate \\( (error_D(h) \\leq \\epsilon)\\); and in time and samples it is bounded by polynomial in \\(  1/\\epsilon, 1/\\delta,\\)and n. </p> <p>If we want perfect error and perfect certainty than the denominators go to infinity as we look at all the data.</p> <p>Something is PAC-learnable if we can learn a hypothesis with low error and high confidence in polynomial time. Sample complexity tells us how many training examples we need to guarantee this level of performance. </p> <ul> <li>True Hypothesis: \\(  c \\in  H \\) : hypothesis aka function</li> <li>Concept: c aka label</li> <li>Concept Class: The class from which the concept that we are trying to learn comes from.   </li> <li>Hypothesis Space: H : The set of mappings that the learning is going to consider.  </li> <li>Size of the hypothesis space: |H|: n  </li> <li>Distribution over inputs: D </li> <li>Version Space: VS(S) = { h s.t. h \\(\\in H \\text{consistent wrt} S\\) : hypothesis consistent with examples : the function we learned labels the data correctly.   </li> <li>Error Goal: \\(   0 \\leq \\epsilon \\leq 1/2\\) : We'd like the error in the hypothesis that we produce to be no larger than epsilon.  Approximately in PAC  </li> <li>Certainty Goal: \\(  0 \\leq \\delta \\leq 1/2\\) : We might be unlucky and not meet our error goal, \\(  \\delta\\) allows us to set a certainty goal, which means with \\(   P(1 - \\delta)\\), the algorithm has to work. To work, here, means to produce a True Error \\( \\leq  \\epsilon\\). Probably in PAC. \\(   \\delta\\) is our confidence paramater in PAC</li> </ul> <p>Haussler's Theorem (Realizable Case)</p> <p>Let H be a finite hypothesis class, and let h \\(\\in\\) H be a hypothesis consistent with a training set of m examples drawn i.i.d. from an unknown distribution D. Suppose the target function f is h \\(\\in\\) H (i.e., the realizable case).</p> <p>Then for any \\(\\epsilon &gt; 0\\) and \\(\\delta \\in (0, 1)\\), if</p> <p>\\( m \\geq \\frac{1}{\\epsilon} \\left( \\ln |H| + \\ln \\frac{1}{\\delta} \\right), \\)</p> <p>then with probability at least \\(1 - \\delta\\), every hypothesis \\(h \\in H\\)that is consistent with the training data satisfies</p> <p>\\( \\Pr_{x \\sim D} [h(x) \\neq f(x)] \\leq \\epsilon. \\)</p> <p>And for infinite hypothesis class:</p> <p>\\( m \\geq \\frac{1}{\\epsilon} \\left( (8 \\cdot VC |H|) \\cdot \\log_2 + 4\\log_2 \\frac{2}{\\delta} \\right), \\)</p> <p>As VS|H| gets bigger we need more data.</p> <p>So... H is PAC-Learnable \\(iff\\) VC dimension is finite.</p>"},{"location":"ml/#vc-dimensions","title":"VC Dimensions","text":"<p>VC Dimensions help us determine how much data we need to learn effectively even if the hypothesis class is infinite. It expresses how expressive or powerful a model class is in terms of what patterns it can learn: this is a measure of complexity or capacity. </p> <p>Most of the algorithms we use have an infinite hypothesis space: - linear separators: there are infinite number of lines to choose from since m x and b are reals - NN: each weights has an infinite amount of numbers it can be since weights are reals - decision trees with continuous inputs: infinite number of questions we can ask at decision nodes - ...</p> <p>How all of our ML algorithms work is like this: keep track of all hypotheses and keep this version space. Once we see enough examples randomly drawn we know we've epsilon-exhausted the version space. So any hypothesis that is left is going to be ok.</p> <p>But we can't keep track of infinite hypotheses.</p> <p>So we will find a way to keep track of only the hypothesis that can affect the outcome differently. </p> <p>Example: X: {1,2,3,4,5,6,7,8,9,10} H: {h(x) = x \\(  \\leq \\theta \\)}</p> <p>|H| = \\(  \\infty \\)</p> <p>But only \\(  \\theta \\) above 10 is important</p> <p>VC Dimension it the maximum number of points that can be shattered by the hypotheses in that space.</p> <p>Shatter: For every possible labeling (0/1 classification) of the points, there exists a hypothesis (a classifier from your class) that perfectly separates the points according to that labeling.</p> <p>VC dimension depends on the hypothesis class, not strictly the number of input features. Example: Both a circle and a sphere have 2 VC dimensions</p> <p>High VC dimension mean the model can shatter more configurations, increasing the risk of over fit, likewise lower VC dimensions may to under fitting and poor accuracy if they are too low.</p>"},{"location":"ml/#information-theory","title":"Information Theory","text":"<p>In ML we want to know:</p> <ol> <li>How each input relates to y</li> <li>Which input splits our output best, giving us the most information about y</li> </ol> <p>In general every input vector and output vector can be considered a probability density function.</p> <p>Information Theory is a mathematical framework that allows us to compare these density functions.</p> <p>Are input vectors are similar:  mutual information</p> <p>Does this feature have any information: entropy</p> <p>If output is predictable we don't need to communicate. Uncertain, meaningful information is harder to communicate and require more resources.</p> <p>more predictable \\(\\approx\\) less uncertainty \\(\\approx\\) less information \\(\\approx\\) less entropy</p> <p>Example: Which message has more information? Given: - Language L={A,B,C,D} - Each word is represented by 2 bits, and occur with a specified probability.</p> <p>First lets assume all words occur equally:</p> <p>A: 00   25% B: 01   25% C: 10   25% D: 11   25%  </p> <p>We have a sequence spelling BAD: 01 00 11</p> <p>2 bit (1 or 0) symbol representation means we have to ask two yes or no questions per symbol.</p> <p>Think of it as a tree. When we have a new bit coming in it can be 0 or 1 equally. So we ask two question, each \"Is it 1?\"</p> <p>Is it one? Answer:   0 1 Is it one? Answer:0 1   0 1 Conclusion:       A B   C D  </p> <p>Second message:</p> <p>A: 50% B: 12.5% C: 12.5% D: 25%  </p> <p>Since A occurs most frequently we can actually use less than two bits to represent each symbol. How should we represent it?</p> <p>For the tree our first question is \"Is it A?\"</p> <p>Is it A?:           0 1  Semi-conclusion:    A Is it D?:             0 1 Semi-conclusion:      D Is it 1?                0 1                          B C  </p> <p>So A: 0 D: 10 B: 110 C: 111   </p> <p>Since A occurs most frequently and we only have to ask one question to determine if the symbol is A, we can ask less questions overall. We use the expected value to find out how much exactly.</p> <p>\\(\\sum P(\\text{symbol}) \\times \\text{size of symbol}\\): (Entropy) = 1P(A) + 2P(D) + 3P(B) + 3P(C) = 0.5 + 0.5 + 0.375 + 0.375 = 1.75 bits per symbol  </p> <p>= 1.75 bits &lt; 2 bits </p> <p>The second language has more information and less randomness.</p> <p>Note: This technique is called variable length encoding and explains why in morse code some symbols are smaller than others.</p> <p>Entropy</p> <p>Entropy captures the in amount of information contained in the variable by quantifying the uncertainty or unpredictability of the variable. </p> <p>higher entropy \\(\\approx\\) more uncertainty \\(\\approx\\) more information </p> <p>As shown above: \\(\\sum P(\\text{symbol}) \\times \\text{size of symbol}\\)</p> <p>But we need to denote the size of each symbol more properly:</p> <p>\\(\\sum P(\\text{symbol}) \\times \\frac{1}{P(\\text{size of symbol})}\\)</p> <p>\\(H(L) = - \\sum P(s) \\cdot \\log_2 P(s)\\)</p>"},{"location":"ml/#information-between-variables","title":"Information Between Variables","text":"<p>Having information about multiple variables can change their probability.</p> <p>If I hear thunder, it might change my prediction about rain and vice versa.</p> <p>Joint Entropy The amount of uncertainty contained in two variables together:</p> <p>\\(H(x,y) = - \\sum P(x,y) \\cdot \\log P(x,y)\\)</p> <p>Joint Entropy \\(H(y|x) = - \\sum P(x,y) \\cdot \\log P(y|x)\\)</p> <p>Mutual information measures the amount of shared information between two variables, by quantifying the reduction in uncertainty of one variable given knowledge of the other \u2014 i.e., statistical dependence.</p> <p>I(X;Y) = H(Y) - H(Y|X)</p> <p>It is symmetric: </p> <p>I(Y;X) = I(X;Y) = H(Y)\u2212H(Y\u2223X) = H(X)\u2212H(X\u2223Y)</p>"},{"location":"ml/bayes_learning/","title":"Bayesian Learning Overview","text":"<p>In machine learning we are trying to learn the \"best\" hypothesis that we can, given some data and some domain knowledge. We do this by searching the hypothesis space. In Bayesian learning we think of best as the most probable or the most likely to be correct. </p> <p>\\(P(h \\in H | Data)\\)</p> <p>\\(\\arg\\max_{h \\in H} P(h \\mid D)\\)</p>"},{"location":"ml/bayes_learning/#bayes-rule","title":"Bayes Rule","text":"<p>\\( P(h \\mid D) = \\frac{P(D \\mid h) \\cdot P(h)}{P(D)} \\)</p> <p>Posterior \\(P(h \\mid D)\\): Probability of hypothesis h given data  \ud835\udc37 D</p> <p>Likelihood \\(P(D \\mid h)\\): Probability we see some data given h is true. Usually easy to find.</p> <p>Prior \\(P(h)\\): Our belief about the hypothesis before seeing the data.</p> <p>Evidence \\(P(D)\\): Prior on the data. It represents the overall probability of observing the data under all possible hypotheses. It is used to normalize the posterior probabilities, ensuring they sum to 1. Often, it doesn't matter when comparing hypotheses for classification because it is constant with respect to h, and thus cancels out in \\(\\arg\\max_h P(h \\mid D)\\).</p> <p>Bayes\u2019 Rule allows us to invert a conditional probability, switching from \\(P(h \\mid D)\\) to \\(P(D \\mid h)\\), so that we can reason about a hypothesis even when \\(P(h \\mid D)\\) is hard to compute directly. This works because of the definition of conditional probability and the symmetry of joint probability:</p> <p>\\( P(D,h) = (P(D \\mid h) P(D) = P(h \\mid D) P(D) \\)</p> <p>This lets us rewrite problems in terms of what is easier to estimate or known, by simply moving the denominator to isolate the posterior.</p>"},{"location":"ml/bayes_learning/#priors-matter-example","title":"Priors Matter: Example","text":"<p>Consider a person diagnosed with an extremely rare disease that occurs in 8 of 1000 people. The test used to diagnose them returns correct positives 97% of the time, and correct negatives 98% of the time. Even though they were diagnosed with the disease, do they actually have it?</p> <p>Given:</p> <ul> <li> <p>Prior probability of disease:   \\( P(\\text{Disease}) = \\frac{8}{1000} = 0.008 \\)</p> </li> <li> <p>Probability of positive test given disease (True Positive rate):   \\( P(\\text{Positive} \\mid \\text{Disease}) = 0.97 \\)</p> </li> <li> <p>Probability of negative test given no disease (True Negative rate):   \\( P(\\text{Negative} \\mid \\text{No Disease}) = 0.98 \\)</p> </li> <li> <p>Therefore, False Positive rate:   \\( P(\\text{Positive} \\mid \\text{No Disease}) = 0.02 \\)</p> </li> </ul> <p>We want to compute the posterior probability that a person has the disease given a positive test:</p> <p>\\( P(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})} \\)</p> <p>\\( P(\\text{Disease} \\mid \\text{Positive}) = 0.97 \\cdot 0.008 \\approx 0.00776 \\)</p> <p>vs</p> <p>\\( P(\\text{not Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{not Disease}) \\cdot P(\\text{not Disease})}{P(\\text{Positive})} \\)</p> <p>\\( P(\\text{not Disease} \\mid \\text{Positive}) = 1 - 0.98 \\cdot 1 - 0.008 \\approx .01984 \\)</p> <p>Since \\( 0.01984 &gt; 0.00776 \\), using argmax we choose the second hypothesis: Even with a positive test result, it is more likely that the person does not have the disease because the disease is so rare. </p>"},{"location":"ml/clustering/","title":"Clustering","text":"<p>Clustering is a way to describe unlabeled by putting objects into groups or clusters. Clusters can help uncover hidden structure, and may align with true labels in structured datasets.</p> <p>Features scaling ensures all features have equal importance and  is essential for all three algorithms below since similarity is measured by \"distance.\" Euclidean distance for K-Means and SLC; and for EM distance is measured using Gaussian densities, where unscaled features skew covariance estimation and likelihood. </p> <p>All three methods are highly sensitive to outliers, especially SLC.</p>"},{"location":"ml/clustering/#single-link-clustering-slc","title":"Single-Link Clustering (SLC)","text":"<p>SLC is a type of hierarchical agglomerative clustering, similar to a spanning tree.</p> <p>Steps: 1. Each datapoint is assigned to its own cluster. 2. At each set, merges the two clusters whose closest pair of points have the smallest distance. 3. Stop when k clusters have formed.</p> <p>Forms elongated chain-like clusters. Can handle non-spherical shapes but is sensitive to noise and outliers.</p> <p>Complete-Link clustering instead merges based on the maximum distance between points in two clusters, creating compact, spherical clusters. Average-Link clustering uses the average distance between all pairs of points across two clusters, providing a balance between chaining and compactness.</p>"},{"location":"ml/clustering/#k-means-clustering","title":"K-Means Clustering","text":"<ul> <li>K-Means is an unsupervised learning algorithm for partitioning data into K clusters.  </li> <li>Each cluster is defined by its centroid (mean of the points in the cluster).  </li> <li>The algorithm minimizes the within-cluster sum of squared distances:  </li> </ul> <p>\\( \\sum_{i=1}^K \\sum_{x \\in C_i} | x - \\mu_i |^2 \\)</p> <p>Steps: 1. Initialize K centroids (randomly or with heuristics like K-Means++). 2. Assign each point to the nearest centroid. 3. Update centroids as the mean of assigned points. 4. Repeat steps 2\u20133 until convergence (no changes or small error).  </p> <ul> <li>Sensitive to initial centroids and may converge to local minima, random restarts help.</li> <li>Doesn\u2019t model probability or handle overlapping clusters well.  </li> <li>Works best with spherical, equally sized clusters, which can be a problem as shown in the image below.</li> </ul> <p></p>"},{"location":"ml/clustering/#expectation-maximization-em","title":"Expectation-Maximization (EM)","text":"<ul> <li>EM is a probabilistic algorithm for maximum likelihood estimation with latent variables.  </li> <li>Commonly used for Gaussian Mixture Models (GMM).  </li> <li>Assumes each data point was generated by a hidden distribution (e.g., one of several Gaussians).  </li> </ul> <p>Steps: 1. E-step: Assign soft responsibilities (probabilities) to each point for belonging to each cluster, based on the current parameters (means, covariances, weights). Not a hard assignment like in K-Means.  2. M-step: Update cluster parameters to maximize the expected likelihood, by averaging over all data points weighted by the responsibilities from the E-step. 3. Iterates E and M steps until convergence.  </p> <ul> <li>Sensitive to initial centroids and may converge to local minima, initializing with K-Means centroids and random restarts can help.  </li> <li>Models soft assignments (fractional membership in clusters).  </li> <li>Can model elliptical, overlapping clusters.  </li> <li>More flexible than K-Means, but computationally heavier.   </li> </ul> Algorithm Runtime (Typical) Best For Single-Link Clustering \\( \\mathcal{O}(n^2 \\log n) \\) or \\( \\mathcal{O}(n^3) \\) Finding non-spherical, chain-like clusters; dendrogram-based analysis K-Means \\( \\mathcal{O}(nkdT) \\) Fast clustering of spherical, well-separated clusters in large datasets EM (e.g., for GMM) \\( \\mathcal{O}(nkd^2T) \\) Soft clustering; modeling overlapping, elliptical clusters with uncertainty <p>K-Means is a simplified version of EM for GMMs, where all clusters have identical, fixed variance and points are hard-assigned to the nearest centroid.</p>"},{"location":"ml/decision_trees/","title":"Decision Trees","text":"<p>Decision Trees are a form of classification learning. Decision Trees are representations of our features and we need to determine representation before deciding on algo. First we identify our problem (root question). Then we identify attributes/features and ask questions about those. These are the decision nodes. Examples: Is it raining? What type of restaurant is it? </p> <p>Edges represent the value of each decision node. Examples: Yes and No. Pizza, Thai, Mexican. </p> <p>The leaf is the answer to the problem (root question), output. Example: Should we each in this restaurant? Should I play tennis? Leaf is yes or no.</p> <p>We need to build a decision tree as a result of processing training data. The order of our decision nodes should correlate to the node's ability to reduce space (number of nodes?). All nodes/features may not be necessary to make our root decision. If we only want to play tennis if its not raining, that might be near the top of our tree as it can rule out other branches and other features.</p> <p>Naive Decision Tree algo (to build a tree) 1. Pick the \"best\" feature \u2014 ideally one that splits the data in half or reduces entropy the most 2. Ask a question based on this feature 3. Follow the answer path 4. If not a leaf node, go back to step 1 with the remaining features and data subset.</p> <p>Problem here is that we have to follow all possible paths and think of all possible best next features until we can completely answer any question. So we aren't learning the tree, we are using a brute-force approach to build it.</p> <p>Given n boolean features, there are \\(2^n\\) possible ways to arrange the features and \\(2^{2^n}\\) ways to assign binary labels to those combinations. The hypothesis space (\\(2^{2^n}\\)) of the decision tree is very expressive because there are many possible functions a decision tree could represent: it has the capacity to represent a massive number of different functions from inputs to outputs.</p>"},{"location":"ml/decision_trees/#id3-algo","title":"ID3 Algo","text":"<p>Loop: 1. A = 'best' feature based on Best Information Gain. 2. Assign A as decision feature at this node (ask a question) 3. For each \\(v \\in A\\), create a branch from current node 4. Group the training examples where feature A = v into the corresponding branch 5. If all examples in a branch are of the same class (pure leaf), stop 6.  Else, repeat the process recursively on that branch with remaining features  </p> <p>Example: A is Weather, values are {Sunny, Rainy, Overcast}. A becomes, \"What is the Weather?\" and \\(v \\in A\\) is one of the possible answers.</p> <p>Information Gain measures how much knowing the value of a feature helps us predict the label of a data point. - Before the split on a feature, our data might be a mix of labels (50% Yes, 50% No ~ high uncertainty) - After the split, if a feature separates the data such that one side is mostly Yes and the other mostly No, we\u2019ve reduced our uncertainty about the label by asking about that feature.  </p> <p>That's the whole goal of decision trees: to use features to reduce label uncertainty step by step.</p> <p>\\( \\text{Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v) \\)</p> <p>Where: - S is current set of training examples - A is the feature we are evaluating (Weather) - Values(A) are the possible values of feature A ({Sunny, Rainy, Overcast}) - \\(S_v\\) is subset of S where feature A = v - Entropy(S) is the entropy of the full set</p> <p>Everything after the minus sign in the Information Gain formula represents the expected entropy after the split: the average uncertainty over each subset, weighted by the proportion of examples in that subset.</p> <p>Entropy is the measure of randomness.</p> <p>Example: A coin has a 50% chance of producing either outcome, so it has 1 bit of entropy. Vs a coin with two heads, which as no entropy or zero bits.</p> <p>\\( Entropy(S) = -\\sum_{v \\in V} p(v) \\cdot \\log_2 p(v) \\)</p> <p>\\( \\text{BestFeature} = \\underset{A}{\\arg\\max} \\ \\text{Gain}(S, A)  \\quad \\text{(i.e., feature that reduces entropy the most)} \\)</p>"},{"location":"ml/decision_trees/#inductive-bias","title":"Inductive Bias","text":"<p>There are two kinds of bias we need to consider when designing any classifier:</p> <p>Restriction Bias: H This bias arises automatically from our choice of hypothesis set, H. It limits the functions we are capable of learning. When using decision trees, H includes only functions that can be represented by decision trees: this is our restriction bias.</p> <p>Preference Bias \\( h \\in H \\) Given \\( h \\in H \\), this is the bias toward which hypotheses within H we prefer. It influences how we search within H, and determines which solutions we favor when multiple hypotheses fit the data.</p> <p>So which decision would ID3 prefer: - Good splits near the top: ID3 greedily chooses features with the highest information gain from the top down.  - Correct over incorrect: ID3 expands nodes until all training examples are correctly classified. - Shallow trees (implicitly): Because it prioritizes informative splits early, ID3 tends to prefer trees with shallower depth, even though this is not explicitly enforced.</p>"},{"location":"ml/decision_trees/#continuous-features","title":"Continuous Features","text":"<p>ID3 attempts to split on every possible value for \\(v \\in A\\). This is infeasible for many features especially continuous ones. The solution is to discretize or bin values. Example: If A = age, we could create a value like v = 20 &lt; Age &lt; 30.</p>"},{"location":"ml/decision_trees/#stopping-point","title":"Stopping Point","text":"<p>ID3 stops expanding the tree when all training examples are perfectly classified: when every leaf is pure. This may lead to very deep trees and overfitting, especially if the data contains noise or exceptions.</p> <p>Example: Suppose Alice plays tennis when it\u2019s raining and windy, but Tony does not. If these cases contradict, the naive ID3 algorithm would keep splitting forever in an attempt to fit both.</p> <p>Solution: Validation-based early stopping Hold out a validation set. Each time the tree expands, evaluate performance on the validation data. If adding a split doesn\u2019t reduce validation error meaningfully, stop expanding. This prevents overfitting and infinite splitting on noisy examples.</p>"},{"location":"ml/decision_trees/#regression","title":"Regression","text":"<p>While classification predicts discrete labels (yes or no), regression predicts continuous numerical values (temperature, price, or age). While classification trees aim to minimize impurity (like Gini or entropy), regression trees aim to minimize a continuous loss function \u2014 typically Mean Squared Error (MSE) \u2014 to better predict numeric outcomes. While classification trees typically use information gain to decide splits, regression trees minimize a continuous loss function such as Mean Squared Error (MSE).</p> <p>\\( \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 \\)</p> <p>MSE predicts the average value of the target variable in each leaf and chooses splits that minimize the variance of the output.</p> <p>During training, we do use MSE to decide where to split. But once the tree is built, and we\u2019re assigning predictions to leaves: - We don\u2019t need to optimize MSE again. - We can just assign the mean of the targets in that leaf.  </p> <p>This is equivalent to minimizing MSE within each leaf.</p>"},{"location":"ml/decision_trees/#lazy-vs-eager","title":"Lazy vs Eager","text":"<p>ID3 is an eager algorithm that builds a full decision tree during training, before it sees test data. While a lazy algorithm delays computation until a predication is needed.</p> <p>A lazy version of ID3 would store the training data and construct just enough of the decision tree on-demand at prediction time to classify the input. This avoids constructing the entire tree ahead of time.</p>"},{"location":"ml/decision_trees/#pruning","title":"Pruning","text":"<p>Pruning removes unnecessary branches/nodes, resulting in a smaller, more interpretable tree without significantly reducing accuracy, hence it simplifies the model. Because of this reduction in complexity, pruning can reduce overfitting.</p> <p>Pruning can affect both breadth and depth.</p> <p>Branches that provide little prediction power, meaning they contribute minimally to reducing classification error, are pruned. Pruning is often error-based or impurity-based (entropy) thresholds.</p> <p>Pruning does not aim to balance the tree structurally, not concerned with symmetry.</p>"},{"location":"ml/feature/","title":"Features","text":"<p>Curse of dimensionality: the amount of data we need grows exponentially in the number of features we have.</p>"},{"location":"ml/feature/#feature-selection","title":"Feature Selection","text":"<p>Create a subset of features needed for our learner. Can be used in supervised or unsupervised learning.</p> <p>Algorithms Families: Filtering and Wrapping  </p> <p>Filtering Search process directly reduces the dataset to a smaller feature set. Faster, but ignore bias.</p> <p>Decision Trees are essentially a filtering algorithm since we split on information gain.</p> <p>Wrapping The search process interacts directly with the learning algorithm to iteratively adjust the feature set based on performance. It considers model bias and performance, making it more accurate but often slower than filter methods.</p> <p>Random optimization algorithms (like GA, SA) work well in this context by treating the learner\u2019s output (e.g., accuracy) as a fitness function, guiding the search toward better feature subsets.</p>"},{"location":"ml/feature/#feature-transformation","title":"Feature Transformation","text":"<p>These techniques are unsupervised </p> <p>Principal Component Analysis (PCA) PCA is a linear transformation technique that projects data into a new space defined by the directions (principal components) of maximum variance. It reduces dimensionality while preserving as much of the total variance as possible. Components are uncorrelated and ranked by the amount of variance they explain. PCA enforces orthogonality to capture variance. </p> <p>PCA can prevent classification if the original feature is best, but the variance is small. PCA will throw away the original feature and we are left with ran data point that doesn't predict.</p> <p>Independent Component Analysis (ICA) ICA transforms the data into components that are statistically independent, not just uncorrelated. It\u2019s often used when the goal is to separate mixed signals (e.g., audio source separation). Unlike PCA, ICA doesn't prioritize variance \u2014 it assumes underlying signals are non-Gaussian and independent.</p> <p>ICA maximization of statistical independence is a stronger condition than uncorrelated-ness. </p> <p>Randomized Component Analysis (RCA) RCA is a transformation method that uses random projections to reduce dimensionality. It preserves pairwise distances approximately using random linear mappings, and is especially useful for very high-dimensional data where exact methods like PCA are computationally expensive. RCA trades accuracy for speed.</p> Method Used For Pros Cons PCA (Principal Component Analysis) Dimensionality reduction, noise filtering, visualization Captures max variance; uncorrelated components; fast &amp; widely used May discard predictive features with low variance; ignores labels ICA (Independent Component Analysis) Blind source separation (e.g., audio signals), uncovering hidden independent factors Finds statistically independent components; useful for signal separation Sensitive to noise; assumes non-Gaussianity; slower than PCA RCA (Randomized Component Analysis / Projection) Fast approximation for high-dimensional data; efficient dimensionality reduction Scales well to large data; preserves pairwise distances approximately Random \u2014 may lose interpretability or useful structure; less accurate"},{"location":"ml/instance_based/","title":"Instance Based Learning","text":"<p>Instance-based learning stores training examples and makes predictions by comparing new inputs to these stored instances (data points) using similarity measures. It defers generalization until query time, earning the label \"lazy learning.\" Unlike model-based learning, which discards the training data after learning a function (e.g., weights), instance-based methods do not learn an explicit model in advance, instead they generalize by referencing stored data at prediction time.</p> <p>Training time is minimal or non-existent. KNN does not require linear separability. Its decision boundaries can be highly nonlinear, depending on the structure of the data and the value of k.</p> <p>They are prone to overfitting, because they are sensitive to noise: they keep all of the noise. Irrelevant features can distort similarity calculations.</p> <p>Inference can be slow, as it often requires comparing the query instance to man or all training examples.</p>"},{"location":"ml/instance_based/#knn-k-nearest-neighbors-algorithm","title":"KNN: k-Nearest Neighbors Algorithm","text":"<p>Good for non-linearly separable problems with moderate dimensionality.</p> <p>Given: - Training data: D = \\({(x_1, y_1), \\dots, (x_n, y_n)} \\) - Query point: \\( x_q \\) - Number of neighbors: k  </p> <p>Steps: 1. For each training point \\( x_i \\in D \\), compute the distance to \\( x_q \\) 2. Sort the training points by distance to \\( x_q \\) 3. Select the k closest points to \\( x_q \\) 4. Let \\( N_k(x_q) \\) be the set of labels of the k nearest neighbors 5. If classification:    - Return the majority label in \\( N_k(x_q) \\) 6. If regression:    - Return the average value in \\( N_k(x_q) \\)</p> <p>Preference Bias: 1. Locality: Assumes near points are similar, so it is important to find the right distance function d() based on domain knowledge. 2. Smoothness: Predicts new values by averaging (or voting over) neighbors, assuming gradual changes in the target function. 3. Relevance: Assumes all features contribute equally unless weighting or feature selection is applied.  </p> <p>Distance Functions: Use Euclidean distance when your features are continuous and you care about straight-line distance in geometric space (e.g., image pixel values or spatial data).</p> <p>Use Manhattan distance when your features are on grids, categorical/ordinal, or when you expect movement in axis-aligned paths (e.g., city block distances, text frequency counts).</p>"},{"location":"ml/rand_optim/","title":"Random Optimization","text":"<p>Random optimization methods search for optimal solutions by incorporating randomness into the search process, rather than relying on gradients or deterministic rules. These techniques are especially useful for non-differentiable, noisy, or high-dimensional objective functions and data with many local optima, where traditional optimization methods struggle. They aim to balance exploration of the search space and exploitation of good candidates, often using heuristics or probability driven decisions.</p>"},{"location":"ml/rand_optim/#randomized-hill-climbing-rhc","title":"Randomized Hill Climbing (RHC)","text":"<p>A simple local search algorithm that starts with a random solution and iteratively makes small random changes that move it towards the optima. If the change improves the objective, it\u2019s accepted; otherwise, it's rejected. Fast and intuitive, but prone to getting stuck in local optima. It can randomly restart to avoid getting stuck in local optima.</p> <p>Steps: 1. Initialize a random guess \\(x \\in X \\) 2. Generate a neighbor \\( n \\in \\mathcal{N}(x) \\) (where \\( \\mathcal{N}(x) \\) is the neighborhood of \\( x \\)) 3. If \\( f(n) &gt; f(x) \\), then set \\( x \\leftarrow n \\) and go back to step 2 4. Else, consider \\( x \\) a local optimum 5. Optionally: Restart from a new random guess and repeat  </p>"},{"location":"ml/rand_optim/#simulated-annealing-sa","title":"Simulated Annealing (SA)","text":"<p>Like hill climbing, but occasionally accepts worse solutions to escape local optima. The probability of accepting worse solutions decreases over time according to a temperature schedule, inspired by the annealing process in metallurgy. Balances exploration and exploitation.</p> <p>Steps: 1. Initialize a random guess \\( x \\in X \\) and a high temperature \\( T &gt; 0 \\) 2. Generate a neighbor \\( n \\in \\mathcal{N}(x) \\) 3. If \\( f(n) &gt; f(x) \\), then set \\( x \\leftarrow n \\) 4. Else, accept \\( n \\) with probability: \\( P = \\exp\\left( \\frac{f(n) - f(x)}{T} \\right) \\) 5. Cool down the temperature: \\( T \\leftarrow \\alpha T \\), where \\( \\alpha \\in (0, 1) \\)  </p> <p>Repeat steps 2\u20135 until stopping criteria is met (e.g., \\( T \\) is very small or max iterations reached)</p> <p>The acceptance probability is the \"else\" case is directly related to the difference in fitness between the current solution and the proposed neighbor. If the neighbor is much worse, the exponent becomes very negative, so there\u2019s a lower chance of accepting it. Instead, we stay at the current solution and take our chances trying a different neighbor in the next iteration.</p>"},{"location":"ml/rand_optim/#genetic-algorithms-ga","title":"Genetic Algorithms (GA)","text":"<p>Inspired by biological evolution. Maintains a population of solutions that evolve over time via selection, crossover, and mutation. Good at global exploration and well-suited for discrete and combinatorial problems, but can be computationally expensive.</p> <p>Steps: 1. Randomly initialize a population P of candidate solutions (chromosomes) of size k 2. Evaluate the fitness of each individual in the population, all \\(x \\in P\\) 3. Select parents from the population based on high fitness 4. Crossover: Combine parts of two parents to create offspring 5. Mutate some of the offspring randomly to maintain diversity (exploration) 6. Evaluate fitness of the offspring 7. Replace individuals in the population with the new offspring 8. Go back to step 3 util stopping condition met  </p> <p>Genetic Algorithms use probability at almost every step \u2014 selection, crossover, mutation \u2014 to explore the search space stochastically rather than deterministically.</p>"},{"location":"ml/rand_optim/#mimic-mutual-information-maximizing-input-clustering","title":"MIMIC (Mutual Information Maximizing Input Clustering)","text":"<p>A probabilistic optimization method that samples solutions based on a learned probability model. At each iteration, it builds a model from the best samples and uses it to generate better ones. Powerful for problems with interdependent variables, but computationally heavy.</p> Algorithm Best For Not Good At Randomized Hill Climbing (RHC) Fast and simple local search; works well for smooth landscapes with a single peak Easily gets stuck in local optima; poor global exploration Simulated Annealing (SA) Escaping local optima; balancing exploration and exploitation via temperature control Requires careful tuning of temperature schedule; still not great for very rugged landscapes Genetic Algorithm (GA) Exploring large, complex, and discrete search spaces; combinatorial optimization Can be slow to converge; sensitive to hyperparameters like mutation and crossover rates MIMIC Modeling dependencies between variables; structured problems with correlated inputs Computationally expensive; not suitable for very large or high-dimensional search spaces"},{"location":"ml/svm/","title":"Support Vector Machines (SVMs)","text":"<p>Support Vector Machines are model-based supervised learning algorithms used for classification and regression. They are linear classifier with a kernel trick. Unlike instance-based methods, SVMs learn an explicit decision boundary by optimizing a global objective during training. They aim to find the hyperplane that maximally separates classes with the largest margin. This leaves the biggest margin of error if some hidden dots got discovered. </p> <p></p> <p>Although SVMs build a global model, only a subset of the training examples, support vectors are used to define the decision boundary. The number of support vectors increases naturally when the data is more complex or less separable, reflecting greater model expressiveness.</p> <p>Support vectors are key data points, not features or dimensions. The number of support vectors reflects how \u201chard\u201d the boundary is to define, but it does not correspond to added dimensions in feature space. Kernel functions, not support vectors, are what increase dimensionality.</p> <p>SVMs work well for both linearly and non-linearly separable data by using the kernel trick, which implicitly maps data into higher-dimensional spaces where linear separation is possible.</p> <p>SVMs can overfit if the kernel or regularization parameter is poorly chosen, but they are generally more robust to noise than instance-based methods, since irrelevant data points that do not affect the margin are ignored.</p> <p>Training time can be high due to solving a convex optimization problem, but inference is efficient and depends only on the number of support vectors.</p> <p>Decision trees can handle linearly separable data, but they are not optimal because they use axis-aligned splits and may require many steps to approximate a linear boundary. Algorithms like logistic regression or linear SVM are typically more efficient in these situations.</p>"},{"location":"ml/svm/#svm-basic-idea","title":"SVM: Basic Idea","text":"<p>Given:  </p> <ul> <li> <p>Training data: \\( D = {(x_1, y_1), \\dots, (x_n, y_n)} \\), where \\( y_i \\in {-1, +1} \\)  </p> </li> <li> <p>Input feature space: \\( R^d \\)  </p> </li> <li> <p>Optional kernel function: \\( K(x_i, x_j) \\)  </p> </li> </ul> <p>Steps:  </p> <ol> <li> <p>Maximize the margin between classes by solving:    \\( \\min_{w, b} \\frac{1}{2} |w|^2 \\quad \\text{subject to } y_i(w^\\top x_i + b) \\geq 1 \\)</p> </li> <li> <p>For non-linearly separable data, add slack variables and regularization:    \\( \\min_{w, b, \\xi} \\frac{1}{2} |w|^2 + C \\sum \\xi_i \\)</p> </li> <li> <p>Use the kernel trick to replace dot products:    \\( K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j) \\)</p> </li> <li> <p>Prediction function:    \\( f(x) = \\text{sign} \\left( \\sum_{i \\in \\text{SV}} \\alpha_i y_i K(x_i, x) + b \\right) \\)</p> </li> </ol>"},{"location":"ml/svm/#preference-bias","title":"Preference Bias","text":"<ol> <li>Maximum Margin: Prefers decision boundaries that maximize distance from the closest points.  </li> <li>Support Vector Focus: Only the most critical training points (support vectors) define the boundary.  </li> <li>Kernel Assumption: Assumes data can be linearly separable in some higher-dimensional space.  </li> </ol> <p>SVMs resemble instance-based methods (like KNN) in that they rely only on critical training examples at prediction time. But unlike KNN, SVMs perform model-based optimization to identify and retain only the important instances (support vectors), discarding the rest.</p>"},{"location":"nlp/distributional_semantics/","title":"Distributional Semantics","text":"<ul> <li>Word embeddings</li> <li>Word2Vec and Skip-gram</li> <li>CBOW</li> <li>GloVe</li> <li>Cosine similarity</li> <li>Analogy tasks</li> <li>Contextual embeddings (BERT, ELMo, etc.)</li> </ul>"},{"location":"nlp/distributional_semantics/#continuous-bag-of-words-cbow","title":"Continuous Bag of Words (CBOW)","text":"<p>Predict a single word based on surrounding words.</p> <p>'crown with the ___ on the throne'</p> <p>Loss: Cross entropy between the predicted distribution and the true center word (i.e., the word that was masked out).</p>"},{"location":"nlp/distributional_semantics/#skip-gram","title":"Skip-gram","text":"<p>Predict surrounding words based on single word.</p> <p>'___ ___ ___ queen ___ ___ ___'</p> <p>Loss: Sum of cross entropy losses for each context word given the center word.</p>"},{"location":"nlp/distributional_semantics/#word2vec","title":"Word2Vec","text":"<pre><code>class CBOW(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(CBOW, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: list of indices (window*2)\n    \"\"\"\n    probs = None\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = output.mean(dim=1)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre> <pre><code>class SkipGram(nn.Module):\n  def __init__(self, vocab_size, embed_size):\n    super(SkipGram, self).__init__()\n    self.embedding_layer = nn.Embedding(vocab_size,embed_size)\n    self.linear_layer = nn.Linear(embed_size,vocab_size)\n\n  def forward(self, x):\n    \"\"\" \n    Args: x: component of data: a single token index\n    Returns: probs: log softmax distro over the vocabulary\n    \"\"\"\n    probs = None\n    ### BEGIN SOLUTION\n    output = self.embedding_layer(x)\n    output = F.sigmoid(output)\n    output = self.linear_layer(output)\n    probs = F.log_softmax(output, dim=1)\n    return probs\n</code></pre>"},{"location":"nlp/machine_reading/","title":"Machine Reading","text":"<p>Open information extraction: Extract structured knowledge from unstructured texted without knowing what the entities or relations are in advance. </p> <p>This is unsupervised.</p>"},{"location":"nlp/machine_reading/#distant-supervision","title":"Distant Supervision","text":"<p>We can get away from knowing fixed relationships.  Instead of manually labeling from an existing knowledge base.</p> <p>Positive Example: Every entity is related to another entity in a sentence. Jack loved his sister, Jill, very much. Jack and Jill are the entities connected by the relation, love. </p> <p>Negative Example: No relation between entities. Jack and Jill went up the hill.</p> <p>Open Info Extraction requires learning a way to recognize relations without labeling. The syntax of sentences provide clues about relations.</p> <p>Semantics arise from syntax. \"Jane loves science.\" We get the meaning because we understand Jane is entity, science is a concept, and love is their positive affinity relationship.</p> <p>Part of Speech (POS) tagging. Assign a POS tag to each word in a sentence. Dynamic  Bayesian Network: Everything in the top row (tags) are latent properties of the document: they don't really exist. If tag is VRB we tend to see certain words we recognize as verbs, so the relationship is an 'emission.' P(word|tag). If a certain tag is present then it emits certain words with greater probability.  It's a dynamic bayesian network because we have this tag1 &gt; tag2 relationship, sequentially. We tend to see certain things follow other tags. P(tag2|tag1) Then we search for a set of tags that maximized the joint probability  1. Time proceeding through network tag1 &gt;  tag2 &gt;  tag3  v       v       v word1   word2   word3</p> <p>Dynamic Programming: We have words, not tags. Compute most likely explanation MLE: What sequence of latent values would most likely emit the observed sequence of words. Use the trick of dynamic programming, assume that choosing most likely tag for each time slice results in the most likely explanation for the entire sequence.</p> <p>Dependency Parsing Words have relationships to each other Graph-structured syntactic analysis of sentence that helps us find these relationships. a. root is a verb b arc are dependencies with types, telling us what is related and how. c. shift-reduce parsing 1 build a dependency graph from the bottom up 2. Buffer: data structure: Processes a sentence word by word from a buffer 3. Stack: data structure: put words that we don't know what to do with in the stack. 4. Operations</p> <p>But how do we know which operation (shift, left, or right?): Learn a classifier: supervised learning.  Given: a stack, a buffer, and POS tags Output: shift, left, right and maybe dependency type Build the dataset: 1. distant supervision 2. annotate lots of sentences with dependency graphs/parses manual process (people do it) Run stack-reduced operation forward and backwards using different operations until it matches the annotations we put in. This means backtracking when necessary to find the known graph It predicts the operations we have to do to get the answers according humans We end up with: 1 X: partial parses of sentences (buffer, stack, POS tags), different stages at each moment: Input to classifier. 2. Y: Best shift-reduce operations: labels for the classifier.</p> <p>Dependency Parsing Words have relationships to each other Graph-structured syntactic analysis of sentence that helps us find these relationships. a. root is a verb b arc are dependencies with types, telling us what is related and how. c. shift-reduce parsing 1 build a dependency graph from the bottom up 2. Buffer: data structure: Processes a sentence word by word from a buffer 3. Stack: data structure: put words that we don't know what to do with in the stack. 4. Operations</p> <p>But how do we know which operation (shift, left, or right?): Learn a classifier: supervised learning.  Given: a stack, a buffer, and POS tags Output: shift, left, right and maybe dependency type Build the dataset: 1. distant supervision 2. annotate lots of sentences with dependency graphs/parses manual process (people do it) Run stack-reduced operation forward and backwards using different operations until it matches the annotations we put in. This means backtracking when necessary to find the known graph It predicts the operations we have to do to get the answers according humans We end up with: 1 X: partial parses of sentences (buffer, stack, POS tags), different stages at each moment: Input to classifier. 2. Y: Best shift-reduce operations: labels for the classifier.</p> <p>__</p> <p>Dependency Parsing and Distant Supervision</p> <p>Goal: </p> <p>Analyze the grammatical structure of a sentence by building a dependency graph: a tree where</p> <p>The root is usually a verb</p> <p>Arcs are labeled with dependency types (e.g., subject, object) showing how words relate.</p> <p>How do we build the dependency graph:</p> <p>We use a shift-reduce parser, which processes the sentence bottom-up using:</p> <p>A buffer: words we still need to process</p> <p>A stack: words we've seen but haven't fully connected yet</p> <p>Operations (shift, left-arc, right-arc): actions that modify the stack and buffer and build the graph</p> <p>The problem: How we we know which operation to perform at each step?</p> <p>Solution: Train a classifier using supervised learning.</p> <p>Building the Training Dataset: Distant Supervision Since we need training examples of (stack, buffer, POS tags) mapped to best action, we simulate the parsing process using sentences already annotated with correct dependency trees (i.e., ground truth graphs).</p> <p>Steps:</p> <p>Manually annotated sentences: Humans create gold-standard dependency parses for lots of sentences. These are the raw material for the training data, but not the final form of the training example the classifier sees directly</p> <p>Replay parsing (shift-reduce simulation): We simulate the parsing process forward, trying operations (shift, left-arc, right-arc).</p> <p>Backtracking if necessary: If a wrong operation is chosen, we backtrack and adjust to make sure the sequence of operations leads exactly to the human-annotated tree.</p>"},{"location":"nlp/machine_reading/#create-training-examples","title":"Create training examples:","text":"<p>X (input): Current parser state (stack, buffer, POS tags at that moment)</p> <p>Y (label): The correct next action (shift, left-arc, right-arc, with possibly a dependency label)</p>"},{"location":"nlp/machine_reading/#why-is-this-called-distant-supervision","title":"Why is this called \"distant supervision\"?","text":"<p>We aren't labeling operations directly (no one manually labeled \"shift here\", \"left-arc here\").</p> <p>Instead, we use the distant information (full annotated trees) and infer the correct operations needed to build them.</p> <p>Supervision is based on final desired output, not step-by-step annotations.</p> <p>In short: We use final human-labeled parses to generate supervision over many intermediate parsing states.</p> <p>Given gold parse: \"She eats apples\" \u2192 \"eats\" is root, \"She\" is subject of \"eats\", \"apples\" is object of \"eats.\"</p> <p>The simulation will do (each of the following three is a training example):</p> <p>Stack: [], Buffer: [She, eats, apples] \u2192 Action: shift</p> <p>Stack: [She], Buffer: [eats, apples] \u2192 Action: shift</p> <p>Stack: [She, eats], Buffer: [apples] \u2192 Action: left-arc (nsubj)</p> <p>and so on...</p> <p>Each (stack, buffer, POS) \u2192 action pair becomes a training sample.</p> <p>The original graph tells us what the right action should be at each step \u2014 but the classifier never gets the full graph as input.</p> <p>Instead of making the model predict an entire tree at once (too hard),</p> <p>We teach it moment-by-moment what the best move is,</p> <p>Based on the gold dependency tree created by humans.</p>"},{"location":"nlp/machine_reading/#example","title":"Example","text":"<p>Suppose sentence: \"She eats apples.\"</p> <p>Gold parse says:</p> <p>\"eats\" is root</p> <p>\"She\" \u2192 subject of \"eats\"</p> <p>\"apples\" \u2192 object of \"eats\"</p> <p>Training samples would look like:</p> <p>Partial state (stack, buffer, POS)  Best action (label) Stack: [], Buffer: [She, eats, apples], POS: [...]  shift Stack: [She], Buffer: [eats, apples], POS: [...]    shift Stack: [She, eats], Buffer: [apples], POS: [...]    left-arc (subject) Stack: [eats], Buffer: [apples], POS: [...] shift Stack: [eats, apples], Buffer: [], POS: [...]   right-arc (object)</p> <p>We train on (parser state \u2192 next action) pairs. We build the full graph by stringing together many next actions. The original annotated dependency graph is used only to supervise, not fed in directly.</p> <p>During Inference: We have a new sentence.</p> <p>Start:</p> <p>Stack is empty []</p> <p>Buffer has all the words of the new sentence [The, cat, eats, fish]</p> <p>POS tags are already assigned (by a separate POS-tagger)</p> <p>At each step:</p> <p>The parser looks at the current stack + buffer (+ POS tags)</p> <p>Uses the trained classifier to predict the best action: shift, left-arc, or right-arc (and maybe dependency label)</p> <p>Apply the predicted action:</p> <p>If classifier says shift: move word from buffer to stack</p> <p>If left-arc: create an arc and pop the second-to-top word from the stack</p> <p>If right-arc: create an arc and pop the top word from the stack</p> <p>Repeat:</p> <p>Keep feeding the new parser state into the model</p> <p>Keep taking actions</p> <p>Until the buffer is empty and only one item remains on the stack (the root)</p> <p>Result:</p> <p>You have a dependency graph for the whole sentence!</p> <p>By dependency parsing the arc map we can make tuples from the unstructured sentences , because it walks us through the graph of the sentence. <p>Put these tuples in the knowledge base to tell us the different ways the entities are related to each other. This makes a structured database.</p> <p>If we don't have perfect matches we use distributional approaches to match questions to relations. Cosine similarity helps us decide if this is the best tuple: we use cosine similarity between different parts of the sentence and pieces of info we have in the tuple.</p> <p>Parse a sentence \u2192 build a dependency graph (using your shift-reduce parser).</p> <p>Walk the dependency graph to extract meaningful (subject, relation, object) triples:</p> <p>Subject = the entity doing the action</p> <p>Relation = the action or link</p> <p>Object = the entity receiving the action</p> <p>\u2192 Example: Sentence: \"The cat eats fish.\" Dependency graph shows:</p> <p>cat (subject) \u2192 eats (verb)</p> <p>eats \u2192 fish (object) Tuple extracted: (cat, eats, fish)</p> <p>Store the triples in a knowledge base:</p> <p>Now you have structured, machine-readable data!</p> <p>Entities are related by known relationships.</p> <p>Natural language is messy: different ways to say the same thing.</p> <p>Distributional approaches help:</p> <p>Use word embeddings (like Word2Vec, GloVe) to represent words or phrases as vectors.</p> <p>Use cosine similarity between:</p> <p>Words in the question</p> <p>Words in the knowledge base tuples</p> <p>This lets you approximate matches, even if the phrasing isn't exactly the same.</p> <p>Example:</p> <p>Question: \"What does a feline consume?\"</p> <p>Knowledge base: (cat, eats, fish)</p> <p>Even though \"feline\" \u2260 \"cat\" and \"consume\" \u2260 \"eats,\" cosine similarity would reveal they're close!</p> <p>That works for who what where and when questions. But why questions are harder.</p> <p>Events are descriptions of changes in the state of world (actual or implied). We need to get at the semantics of the events because these are not in the text.</p> <p>Frames: Acknowledge that a word's meaning cannot be understood without access to essential knowledge that relates to the world.</p> <p>A Frame is a system that relates concepts such that understanding one concept requires the understanding of all concepts and their relationships</p> <p>circular: seller &gt; sell &gt; recipient &gt; possession &gt; seller ...</p> <p>FrameNet: avenger, punishment, offender, injury, injured-party Made by humans Lexical units can bring up this frame: get back at, get even, avenge...</p> <p>If we can identify the frame and the frame elements, we can make inferences about the relationships between the frame elements that we might not be able to do from just the surface form of the sentence.</p> <p>Sally wanted to get back at Jane after her cat died. Why did sally want to do this? \"We need to understand 'get back at'.</p> <p>VerbNet: Linguist resource of frames centered around verbs.</p> <p>Links syntactic and semantic patterns.</p> <p>Can be used for: Word Sense Disambiguation  Figurative Language Detection</p> <p>VerbNet organizes verbs into classes based on their meaning and syntax.</p> <p>Each verb class defines:</p> <p>What kinds of participants (roles) it expects (Agent, Patient, Instrument, etc.)</p> <p>What kinds of syntactic frames are typical (subject-verb-object, subject-verb, etc.)</p> <p>It also gives semantic roles and predicates that describe the event structure clearly.</p> <p>Key: VerbNet links natural language to structured events and roles.</p>"},{"location":"patterns/","title":"Patterns","text":"<p>Reusable code and structures used across domains.</p>"},{"location":"patterns/#docstring-templates","title":"Docstring templates","text":""},{"location":"patterns/#function-template-google-style","title":"Function Template (Google Style)","text":"<pre><code>def my_function(arg1, arg2):\n    \"\"\"Brief summary of function.\n\n    Params:\n        arg1, type: Description of first argument. (shape)\n        arg2, type: Description of second argument. (shape)\n    Returns:\n        type: Description of the return value.\n    Raises:\n        SomeError: When this happens.\n    \"\"\"\n</code></pre>"},{"location":"patterns/#class-template","title":"Class Template","text":"<pre><code>class MyModel:\n    \"\"\"Short summary of class.\n\n    Attributes:\n        name (str): Description of the attribute.\n        layers (list): Description of the model layers.\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Initialize the model with a name.\"\"\"\n        self.name = name\n</code></pre> <p>--</p>"},{"location":"patterns/#github-workflow","title":"Github Workflow","text":"<p>Note: This doc is based on using the command line (terminal) in VSCode, not the VSCode extension for git.</p>"},{"location":"patterns/#0-clone-the-repo","title":"0 Clone the repo","text":"<p>Go to GitHub in your browser. Go to our team repo and clone the project. Click the green \u201ccode\u201d button and clone the repo however you like.  Copy the link. On the command line type: <code>git clone &lt;paste link&gt;</code></p>"},{"location":"patterns/#01-get-your-bearings","title":"0.1 Get your bearings","text":"<p>cd into the folder created by cloning, then type: <code>ls</code> to see all the files</p>"},{"location":"patterns/#1-ensure-you-are-on-the-main-branch","title":"1 Ensure you are on the Main Branch","text":"<p><code>git checkout main</code> </p>"},{"location":"patterns/#2-pull-any-new-changes-from-main-branch","title":"2 Pull any new changes from Main Branch","text":"<p>Ensure you have the most up-to-date code <code>git pull</code></p>"},{"location":"patterns/#3-create-your-branch","title":"3 Create your branch","text":"<p><code>git branch &lt;YourFeatureHere&gt;</code> example: <code>git branch DogDoorActivator</code></p>"},{"location":"patterns/#4-checkout-your-branch","title":"4 Checkout your branch","text":"<p><code>git checkout &lt;YourFeatureHere&gt;</code> example: <code>git checkout DogDoorActivator</code></p>"},{"location":"patterns/#41-make-sure-you-are-where-you-want-to-be","title":"4.1 Make sure you are where you want to be","text":"<p>You can always type: <code>git branch</code> to make sure you are on the branch you want to be on</p>"},{"location":"patterns/#5-set-the-remote-as-upstream-so-you-can-eventually-push-changes-to-github","title":"5 Set the remote as upstream so you can eventually push changes to GitHub","text":"<p><code>git push --set-upstream origin DogDoorActivator</code></p>"},{"location":"patterns/#6-write-your-code-on-this-branch","title":"6 Write your code on this branch.","text":""},{"location":"patterns/#7-commit-early-push-often","title":"7 \u201cCommit early push often.\u201d","text":"<p>We want to push our commits to the GitHub remote feature branch often. When you are done for the day or want to take a break, commit and push. From the root of the project folder do all three of the following: 1. <code>git add .</code> this tracks all of the files in this directory and its subdirectories  2. <code>git commit -m \u201c&lt;YourCommitMessageHere&gt;\u201d</code> 3. <code>git push</code> this pushes any unsynced commits to remote branch on gitHub</p>"},{"location":"patterns/#71-documentcomment-and-test","title":"7.1 Document/Comment and Test","text":"<p>Comment and test your code before you ask for a PR so the reviewer will have an easier time.</p>"},{"location":"patterns/#8-pull-request-pr","title":"8 Pull Request (PR)","text":"<p>When you are completely done with your feature and you are ready to merge these changes to main, go to the GitHub repo and open a Pull Request (PR) from your branch to Main.</p>"},{"location":"patterns/#9-ask-one-of-your-peers-to-review","title":"9 Ask one of your peers to review.","text":"<p>If the peer has changes for you, do those changes on your branch. If it is perfect, they can merge it for you or approve it for you to merge. Click \u2018squash and merge\u2019 on the PR.</p>"},{"location":"patterns/#10-delete-your-upstream-branch-on-github","title":"10 Delete your upstream branch on gitHub.","text":"<p>Or depending on your workflow, you can keep using your branch.</p>"},{"location":"patterns/#11-repeat-for-your-next-feature","title":"11 Repeat for your next feature!","text":"<p>Starting at step 1.</p>"},{"location":"patterns/#for-this-io-site","title":"For this io site","text":"<p>After pushing repo, on command line <code>mkdocs gh-deploy</code>.</p>"},{"location":"patterns/#general-hyperparameter-tuning-strategy","title":"General Hyperparameter Tuning Strategy","text":"<p>Improve model performance by adjusting key training and model configuration parameters without overfitting or wasting compute.</p> <p>Start with  baseline: - Run model with default or reasonable parameter - Record metrics: accuracy, loss, runtime - Use this to compare future improvements as we gradually increase complexity - Track experiments with Weights &amp; Biases  </p>"},{"location":"patterns/#tuning-order-priority","title":"Tuning Order Priority","text":"<ol> <li> <p>Learning rate</p> <ul> <li>Start with learning rate finder  </li> <li>Often has largest effect on both convergence speed and final performance  </li> </ul> <p>The learning rate controls the size of step your optimizer takes when updating weights based on the gradient. It's arguably the most important hyperparameter to tune, and should be the the first you tune. A good learning rate balances speed and stability of training, IMO. Too small: slow convergence (or stuck in a local minimum).Too big: diverging loss or oscillating weights. Good: convergences fast to a good minimum. You can use a learning rate finder. Start with a very small learning rate, increase it exponentially, track the loss at each step, and plot loss vs. learning rate. The best learning rate is often just before the loss starts increasing rapidly.</p> </li> <li> <p>Batch Size  </p> <ul> <li>Larger batches: more stable gradients but may generalize worse  </li> <li>Smaller batches: more noise, sometimes better generalization</li> <li>Changing batch size may require retuning learning rate  </li> </ul> </li> <li> <p>Architecture/model capacity: decide if model too big/small  </p> <ul> <li>Tune only after learning is stable    </li> <li>Adjust number of layers, hidden units, width vs. depth    </li> </ul> </li> <li> <p>Regularization  </p> <ul> <li>Dropout, weight decay, early stopping, etc.</li> <li>After we have a model that can overfit out data  </li> </ul> <p>Weight decay is a regularization technique that helps prevent overfitting by discouraging large weights in the model. I do regularization tuning after learning rate, batch size, and model capacity. Decay too small: model may overfit because it memorizes training data. Too high: model underfits because it can\u2019t capture patterns. Good: generalizes well by keeping weights. Start with a small weight decay value and gradually increase it until you see overfitting begin to diminish while training loss stays low. Once you find a good balance, it\u2019s often worth restarting the tuning cycle at the learning rate as you might now be able to improve the loss further with a better learning rate in combination with the new regularization setting.</p> </li> <li> <p>Optimizer     _ Different optimizers work better for different problems</p> <ul> <li>Adam is good default </li> <li>Try SGD+momentum is better for CV and large-scale tasks</li> </ul> </li> <li> <p>Advanced hyperparams specific to arch. (attention heads, embedding dims.)</p> </li> </ol>"},{"location":"patterns/#convergence","title":"Convergence","text":"<p>The point where the model stops meaningfully improving </p> <p>Signals of Convergence - Validation loss stops decreasing or begins to increase - Evaluation metric (e.g., accuracy, F1) stops improving - Training loss decreases but validation loss worsens: overfitting - Model predictions stabilize (low variance across epochs)</p> <p>Best Practices - Use early stopping (e.g., patience = 5 epochs without improvement) - Be cautious of slow convergence caused by learning rate that\u2019s too low  </p> <p>Practical Convergence Criteria - Task-specific thresholds that define \u201cgood enough\u201d - Helps guide whether to continue training, stop early, or try a different approach  </p> <p>Examples - \"Reach 80% accuracy in under 5 epochs\" - \"Improve F1 score to 0.7 using fewer than 100K parameters\" - \"Train for no more than 30 minutes on a laptop GPU\" - \"Outperform logistic regression by 5%\"</p>"},{"location":"patterns/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Accuracy - Percentage of correct predictions - Best for: balanced datasets - Bad for: imbalanced classes (e.g., 95% accuracy when 95% of data is class A = misleading)</p> <p>Precision - Of predicted positives, how many were correct? - Formula: <code>TP / (TP + FP)</code> - Best for: when false positives are costly (e.g., spam, security alerts)</p> <p>Recall - Of actual positives, how many were correctly predicted? - Formula: <code>TP / (TP + FN)</code> - Best for: when false negatives are costly (e.g., cancer and fraud detection)</p> <p>F1 Score - Harmonic mean of precision and recall - Formula: <code>2 * (P * R) / (P + R)</code> - Best for: imbalanced data where you care about both P and R</p> <p>AUC-ROC - Probability the model ranks a positive higher than a negative - Best for: binary classification, threshold-based tasks</p> <p>Perplexity - Measures how well a language model predicts a sample - Lower is better: lower perplexity means the model is more confident and assigns higher probability to the correct words  - So, Perplexity of 1 means perfect prediction (model is certain). Perplexity of 10 means model is choosing among ~10 likely options per token. - Best for: language models trained for next-word or sequence prediction (e.g., RNNs, Transformers)</p> <p>\\(   \\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i)} \\)  </p> <p>MSE / MAE / RMSE - Error metrics for continuous outputs - Best for: regression problems</p>"},{"location":"patterns/#batching","title":"Batching","text":"<p>Basic batching</p> <pre><code>def get_batch(data, index, batch_size=10):\n    \"\"\" \n    Create batch of data of given batch_size, starting at given index.\n    Check device, run on GPU if available. \n\n    Args: \n        data: CBOW data, list of (context_indices, target_index) tuples (2 * window, int)\n\n    Returns:\n        x: tensor of context words, one row per training example, each with it\n            context word indices, (batch_size, window * 2)\n        y: tensor of target words, one target word index per row (batch_size,)\n    \"\"\"\n    batch = data[index:index + batch_size]\n    x = [row[0] for row in batch]\n    y = [row[1] for row in batch]\n\n    x = torch.tensor(x, dtype=torch.long).to(DEVICE)\n    y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n    return x, y\n</code></pre>"},{"location":"patterns/glossary/","title":"Glossary","text":""},{"location":"patterns/glossary/#polynomial-time","title":"Polynomial Time","text":"<p>When we say an algorithm runs in polynomial time, we mean that:</p> <p>Its worst-case running time can be expressed as a polynomial function of the input size.</p> <p>Like the following, where $ n $ = input size:</p> <ul> <li>O(n) \u2013 linear time  </li> <li>O(n^2) \u2013 quadratic time  </li> <li>O(n^3) \u2013 cubic time  </li> <li>...  </li> <li>O(n^k) \u2013 for some constant k</li> </ul> <p>Polynomial time is considered efficient. It contrasts with exponential time algorithms, like O(2^n), which become intractable as n grows.</p>"},{"location":"patterns/stats/","title":"Probability &amp; Statistics","text":""},{"location":"patterns/stats/#statistics","title":"Statistics","text":""},{"location":"patterns/stats/#moving-average","title":"Moving Average","text":"<p>A moving average is a way to smooth out noisy data by averaging over a sliding window of points. We lose the first and last few points, but the result is a smoother version of the original signal. Output = N - W + 1. So the first W - 1 data points don\u2019t get included in the result (no \"look-behind\"). A moving average smooths the data by averaging out short-term fluctuations. It dampens highs and lows, suppressing noise.</p> <p>Imagine you have this list of episode rewards:</p> <p>data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</p> <p>If you choose a window size of 3, here\u2019s what happens:</p> <p>Take the average of the first 3 numbers: (1 + 2 + 3) / 3 = 2</p> <p>Slide the window one step and average the next 3: (2 + 3 + 4) / 3 = 3</p> <p>Keep doing this: (3 + 4 + 5) / 3 = 4 (4 + 5 + 6) / 3 = 5 ...</p> <p>[2, 3, 4, 5, 6, 7, 8, 9]</p> <p>Why Use It? In reinforcement learning, the reward per episode can bounce all over the place due to randomness. A moving average helps you see the underlying trend by smoothing out that variance.</p> <p>np.convolve(data, np.ones(window_size) / window_size, mode='valid')</p>"},{"location":"rl/","title":"Reinforcement Learning","text":""},{"location":"rl/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>An MDP is a formal framework used in reinforcement learning to model decision-making in environments with stochastic outcomes. It provides a structured way to reason about states, actions, rewards, and transitions over time. An MDP assumes the Markov property, meaning the future depends only on the current state and action, not the full history.</p> <p>An MDP is defined by the tuple:</p> <p>\\( (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma) \\)</p> <ul> <li>\\( \\mathcal{S} \\): set of possible states </li> <li>\\( \\mathcal{A} \\): set of possible actions </li> <li>\\( \\mathcal{P}(s' \\mid s, a) \\): transition probability   function</li> <li>\\( \\mathcal{R}(s, a) \\): reward function </li> <li>\\( \\gamma \\in [0,1] \\): discount factor, which prioritizes immediate rewards over distant ones  </li> </ul>"},{"location":"rl/#policy-pi","title":"Policy (\\( \\pi \\))","text":"<p>A policy defines the behavior of an agent in a MDP. It maps states to actions, specifying what action the agent should take in each state.</p> <ul> <li> <p>A deterministic policy is a function:   \\( \\pi(s) = a \\)  </p> </li> <li> <p>A stochastic policy gives a probability distribution over actions:   \\( \\pi(a \\mid s) = P(\\text{take action } a \\text{ in state } s) \\)</p> </li> </ul> <p>The goal of reinforcement learning is to find a policy that maximizes expected cumulative reward over time.</p>"},{"location":"rl/#bellman-equation","title":"Bellman Equation","text":"<p>The Bellman Equation expresses the value of a state as the expected return starting from that state and following a given policy. It breaks down the value into immediate reward plus the discounted value of the next state, enabling recursive computation of value functions.</p> <p>For a policy \\( \\pi \\), the Bellman Expectation Equation is:</p> <p>\\( V^\\pi(s) = \\mathbb{E}_\\pi \\left[ R(s, a) + \\gamma V^\\pi(s') \\right] \\)</p> <p>This tells us how to evaluate how good a policy is, but not necessarily which action is best, that requires comparing policies.</p> <p>Output: - A value function:   A table (or function) mapping each state \\( s \\) to a scalar value:   \\(   V^\\pi(s_1) = 3.7,\\quad V^\\pi(s_2) = 1.2,\\quad V^\\pi(s_3) = 5.0, \\ldots   \\)  </p> <ul> <li>This tells you how good it is to be in state \\( s \\) when following policy \\( \\pi \\)</li> </ul> <p>For finding the optimal policy, we use the Bellman Optimality Equation, which chooses the best possible action in each state:</p> <p>For the optimal policy, the Bellman Optimality Equation is:</p> <p>\\( V^\\ast(s) = \\max_a \\mathbb{E} \\left[ R(s, a) + \\gamma V^\\ast(s') \\right] \\)</p> <p>Output: - A value function representing the maximum expected return from each state \u2014 assuming optimal behavior - Same format:   \\(   V^\\ast(s_1) = 6.2,\\quad V^\\ast(s_2) = 2.9, \\ldots   \\)</p> <ul> <li>Often used to derive the optimal policy:   \\(   \\pi^\\ast(s) = \\arg\\max_a \\mathbb{E}[R(s, a) + \\gamma V^\\ast(s')]   \\)</li> </ul>"},{"location":"rl/#value-iteration-vi","title":"Value Iteration (VI)","text":"<p>Value Iteration is a dynamic programming algorithm that finds the optimal value function by repeatedly applying the Bellman Optimality Equation. At each step, it updates the value of every state based on the best possible action, gradually converging to the optimal value function \\( V^* \\).</p> <p>Update rule:</p> <p>\\( V_{k+1}(s) = \\max_a \\mathbb{E} \\left[ R(s, a) + \\gamma V_k(s') \\right] \\)</p> <p>Once \\( V^\\ast \\) has converged, the optimal policy can be extracted by choosing the action that maximizes the expected return:</p> <p>\\( \\pi^\\ast(s) = \\arg\\max_a \\mathbb{E} \\left[ R(s, a) + \\gamma V^*(s') \\right] \\)</p>"},{"location":"rl/#policy-iteration-pi","title":"Policy Iteration (PI)","text":"<p>Policy Iteration is a dynamic programming method that alternates between evaluating a policy and improving it until convergence to the optimal policy \\( \\pi^* \\).</p> <p>It has two main steps:</p> <ol> <li> <p>Policy Evaluation:    Compute the value function \\( V^\\pi(s) \\) for the current policy using the Bellman Expectation Equation.</p> </li> <li> <p>Policy Improvement:    Update the policy by choosing the action that maximizes expected return using \\( V^\\pi(s) \\):</p> </li> </ol> <p>\\(    \\pi'(s) = \\arg\\max_a \\mathbb{E} \\left[ R(s, a) + \\gamma V^\\pi(s') \\right]    \\)</p> <p>Repeat these steps until the policy stops changing \u2014 at that point, \\( \\pi^ \\) is the optimal policy*.</p> <p>More expensive than VI, but less iterations.</p>"},{"location":"rl/#q-learning","title":"Q-Learning","text":"<p>Value Iteration (VI) and Policy Iteration (PI)are used to solve MDPs, but they are not considered true reinforcement learning algorithms, because they assume full knowledge of the environment:</p> <p>They require access to the transition probabilities \\( P(s' \\mid s, a) \\) and the reward function \\( R(s, a) \\).</p> <p>In contrast, reinforcement learning (like Q-learning or SARSA) is designed for situations where the agent must learn by interacting with the environment, without knowing the transition or reward model in advance.</p> <p>Q-learning is an off-policy reinforcement learning algorithm that learns the optimal action-value function \\( Q^*(s, a) \\) without requiring a model of the environment. It updates estimates of Q-values based on observed rewards and the maximum estimated future return.</p> <p>The update rule is:</p> <p>\\( Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] \\)</p> <ul> <li>\\( \\alpha \\): learning rate  </li> <li>\\( \\gamma \\): discount factor  </li> <li>\\( r \\): reward from taking action \\( a \\) in state \\( s \\)  </li> <li>\\( s' \\): resulting next state</li> </ul> <p>Over time, Q-learning converges to the optimal Q-function, and the optimal policy is:</p> <p>\\( \\pi^\\ast(s) = \\arg\\max_a Q^\\ast(s, a) \\)</p> <p>Q-Values represent expected cumulative reward for taking a specific action in a given state.</p> <p>Q-values represent the expected cumulative reward for taking a specific action in a given state, and then following the policy thereafter. So:</p> <p>\\( Q(s, a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid s_0 = s, a_0 = a \\right] \\)</p> <p>Where: - \\( s \\) is the current state - \\( a \\) is the action taken - \\( \\gamma \\) is the discount factor - \\( R_t \\) are the rewards</p> <p>So it\u2019s about both state and action (not just the state).</p> <p>Output: - A table of values for each state-action pair:  \\(   Q^\\ast(s_1, a_1) = 4.5,\\quad Q^\\ast(s_1, a_2) = 2.1,\\quad Q^\\ast(s_2, a_1) = 5.0, \\ldots   \\)</p> <ul> <li>Lets you directly select the best action in each state:   \\(   \\pi^\\ast(s) = \\arg\\max_a Q^\\ast(s, a)   \\)</li> </ul> <p>epsilon decay ration Even if we think a certain action has the highest Q-Value(based on earlier exploration) in a state, we still randomly try other actions with probability epsilon. This helps us discover better actions we might have missed early on.</p> <p>alpha: learning rate Controls how much we update the Q-value based on new information. Higher values is bigger updates to Q-values.</p> <p>Epsilon controls which actions we try (explore).</p> <p>Alpha controls how strongly each experience influences learning. </p> <p>\\( Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\big[ \\text{target} - Q(s, a) \\big] \\)</p> Algorithm Key Idea Pros Cons Value Iteration (VI) Iteratively update value function using Bellman optimality Simple to implement; guaranteed convergence Requires full model; can be slow to converge Policy Iteration (PI) Alternates between policy evaluation and improvement Often faster convergence than VI Requires full model; policy evaluation can be expensive Q-Learning Learn action-value function via experience; off-policy Doesn\u2019t need model; learns while acting Slower convergence; sensitive to exploration strategy <p>The main difference between Value Iteration (VI) and Policy Iteration (PI) is how they update the policy:</p> <p>VI updates the value function first, using the Bellman optimality equation, and derives the policy afterward.</p> <p>PI starts with a policy and alternates between evaluating the current policy and improving it based on the value estimates.</p>"},{"location":"rl/game_theory/","title":"Game Theory","text":"<p>perfect information: we always know what state we are in</p> <p>strategies: mapping of all possible states to actions. Each agent has their own strategy.</p>"},{"location":"trading/","title":"Trading","text":""},{"location":"trading/#types-of-traders","title":"Types of Traders","text":"<p>Day trader\" Buys and sells securities within the same day, closing all positions before the market closes to profit from intraday price movements.</p> <p>Swing trader: Hold positions for a few days to weeks, aiming to profit from short- to medium-term price swings</p> <p>Position trader: Holds positions for months or years, focusing on long-term trends and fundamentals.</p> <p>Scalper: Makes rapid trades within minutes or seconds, profiting from tiny price changes.</p> <p>Volatility trader: Profits from expected changes in price movement, regardless of direction, often using options or volatility indices.</p> <p>Income trader: Focuses on generating regular income from dividends or selling options, rather than capital gains.</p>"},{"location":"trading/#indicators","title":"Indicators","text":"<p>MACD(12,26,9) (Moving Average Convergence Divergence): An oscillator that shows the relationship between two moving averages of the price.</p> <p>MACD Line: Trend Momentum. Bearish if &lt; 0. Calculated by subtracting the 26-period EMA from the 12-period EMA. Signal Line: 9-period smoothed MACD for comparison Histogram: Negative means MACD is below signal so bearish. Represents the difference between MACD line and the signal line, providing insights into momentum.</p> <p>Crossovers:  Bullish signal when the MACD line crosses above signal line, suggesting a potential uptrend may be developing. Bearish signal when MACD line crosses below the signal line, indicating potential downturn.</p> <p>Divergence: When the prices moves in one direction, while the MACD moves in the opposite direction. Bullish divergence: price making lower lows while MACD makes higher lows, can signal a potential price reversal to the upside, especially when combined with a crossover.</p> <p></p>"}]}